---
title: "Top N%"
author: "Megan Lin, James Powell, Eva Mustafic"
date: "5/5/2021"
output: 
  html_document:
    toc: true
    toc_float : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message = FALSE)
library(class)
library(caret)
library(e1071) 
library(tidytext)
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(C50)
library(mlbench)
library(rattle)
library(rpart.plot)
library(rio)
library(plyr)
library(dplyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
```
## KNN

### Reading in the Data

The data has previously been merged and cleaned using Python, but we are going to make sure all of the rows with incomplete data are filtered out. Specifically, we will be filtering out employees with a gender the Python program was unable to find in any existing gender-name dictionaries or was deemed to be a gender-neutral name like Alex.

```{r}
data <- read.csv("SEAS_cleandata.csv")
data <- subset(data, Gender %in% c("F", "M"))
```

### Split the Data
To determine the split, we had to determine the ratio of female to total, and based on that, we can deduce the percent of male entries, and the percentage of both. To do this we created the function ad_split, which prints the ratio of female to total entries and ran it on the clean salary dataset. The split between female to male is 22 percent to 78 percent within the School of Engineering, respectively.

```{r echo=FALSE, include = FALSE, message = FALSE}
# Determine the split between female and male, then calculate the base rate
ad_split <- function(dat){
  female <- filter(dat, Gender == 'F')
  n_comm <- nrow(female)
  n_noncomm <- nrow(dat) - nrow(female)
  n_total <- nrow(dat)
  n_comm/n_total
}
ad_split(data)
#There is a 22/78 split
```

### Choose the Data

To filter out the vars, we create a list of the vars names and saved the dataset over itself without the columns in the vars variable. 
```{r echo=FALSE, message = FALSE}
# We drop all columns that do not contain relevant data
vars <- c('Department', 'Position', 'Tenure', 'Gender', 'Salary')
data_clean <- data[,(names(data) %in% vars)]
data_clean$Department <- as.numeric(as.factor(data_clean$Department))
data_clean$Position <- as.numeric(as.factor(data_clean$Position))
data_clean$Tenure <- as.numeric(as.factor(data_clean$Tenure))
data_clean$Gender <- as.numeric(as.factor(data_clean$Gender))
data_clean$Salary <- as.numeric(gsub("[\\$,]", "", data_clean$Salary))

sapply(data_clean, class) 
```

Afterwards, we just simply had to run the function cor(), to discover how each column was related to each other to. We want to check if there are any highly correlated variables and remove them so as to not skew our KNN. We used a threshold of 0.7 to determine if something was too highly correlated.
```{r}
# Before we run kNN, sometimes it's good to check to make sure that our variables are not highly correlated. Use the cor() function on data_clean, label it 'correlations', and view the data, because kNN doesn't work well in high dimensions. 
correlations <- cor(data_clean)
```

None of the variable are too highly correlated so we will leave these all in. We will note that Tenure and Salary has a correlation of 0.6 so we will keep an eye out for it. 

The good thing we can see here though is that Gender & Salary only have a correlation of 0.15 which is low and indicated that there isn't a glass ceiling we should be worried about if we are looking only at gender. However, there are more things that determine if there is indeed a glass ceiling-- specifically if these employees we are comparing are indeed doing "the same job".

### Generate Training and Testing Sets

```{r echo=FALSE, include = FALSE, message = FALSE}
# Use the index to generate a train and test sets, then check the row counts to be safe. 
# Check the composition of labels in the data set. 

# Let's split the data into a training and a test set.
# Sample 80% of our know data as training and 20% as test.
set.seed(1982)
train_rows <- sample(1:nrow(data_clean),
                              round(0.8 * nrow(data_clean), 0),
                              replace = FALSE)#<- don't replace the numbers


train <- data_clean[train_rows, ] #<- select the rows identified
test <- data_clean[-train_rows, ]  #<- select the rows that weren't identified 
```

#### Data Sets {.tabset}

##### Training Data
```{r }
head(train)
```

##### Testing Data
```{r }
head(test)
```

### Choosing the Best K
```{r echo=FALSE, include = FALSE, message = FALSE}
# Run the "chooseK" function to find the perfect K, while using sapply() function on chooseK() to test k from 1 to 21 (only selecting the odd numbers)
labels<- colnames(data_clean)

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

knn_diff_k_com <- sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train[labels],
                                             val_set = test[labels],
                                             train_class = train[, "Salary"],
                                             val_class = test[, "Salary"]))
# Create a dataframe so we can visualize the difference in accuracy based on K, convert the matrix to a dataframe
trans_knn_diff_k_com<-as.data.frame(t(as.matrix(knn_diff_k_com)))

colnames(trans_knn_diff_k_com)<-c("K", "Accuracy")
trans_knn_diff_k_com <- trans_knn_diff_k_com[order(trans_knn_diff_k_com$Accuracy),] 
trans_knn_diff_k_com
```

Unfortunately, we see that there are no good number of splits if we use KNN so instead we can look at which variable is most important using a decision tree!

## Decision Tree

We will use a decision tree to determine which variable is most important as we sample the top N% of the UVA employees. We will iterate through the top 1 down to the N% where gender is the most significant determining factor for salary.

### Splitting
Next, we split the data into a training and a testing set
```{r}
# Split your data into test and train using the caret
x <- createDataPartition(data_clean$Salary,times=1,p = 0.8,list=FALSE)
training <- data[x,-3]
testing <- data[-x,-3]
```

### Baserate
```{r}
# Ok now determine the baserate for the classifier, what does this number mean.  
#For the multi-class this will be the individual percentages for each class. 

data_long = data_clean %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -Salary) 
data_long_form = ddply(data_long, 
                            .(Var, Value),#<- group by Var and Value, "." 
                            #allows us to call the variables without quoting
                            summarize,  
                            prob_Salary = mean(Salary), #<- probability of being Parent
                            prob_not_Salary = 1 - mean(Salary)) #<- probability of not being Parent
```

## Building the Model

We build our model using the default setting and by using the rpart function. PR.Status is used as the "formula" aka our response variable. We utilize our previouslt split training dataset and set a cp of 0.01 as it is our default.

```{r}
#7 Build your model using the default settings
set.seed(2702)
data_gini = rpart(Salary~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))
```

### Variable Importance

```{r}
data_gini$variable.importance
```
We can also visualize the importance of including each variable as shown above. This reiterates similar points to what we have previously found, but displays the relative importance for all of the variables now. Most important is the Converted Stage with a top score of 3.24 followed by AJCC stage with a score of 3.21. At the bottom and being the least important are node coded with a score of 0.22 and metastasis than 0.185.
---
title: "Final Project"
subtitle: "UVA SEAS Salary and Gender" 
author: "Megan Lin, James Powell, Eva Mustafic"
date: "5/5/2021"
output: 
  html_document:
    theme: spacelab
    toc: true
    toc_float : true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = TRUE, message = FALSE)
library(class)
library(caret)
library(e1071) 
library(tidytext)
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(C50)
library(mlbench)
library(rattle)
library(rpart.plot)
library(rio)
library(plyr)
library(dplyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
```


## Question of Interest

For our analysis we were interested to see if there was a difference between male and female when it comes to salaries at UVA. In order to explore that, we started of with a question:

1. Is gender a top determining factor within the top N% of faculty at UVA?

For that purposes we decide to use KNN model and explore this question further. But as we will explain later, we did not have many answers to that question, so we were determined to change it. Or follow up question was:

2. Can we predict the gender based on salary (as well as other variables)?

For that purposes we decided to use Decision Tree model and as we will explain later, had more success in explaining what role does gender play at UVA faculty positions. 




## Data 

The data we used to do our initial background research was the {r} [Cavalier Daily's 2020 Faculty Salaries] (https://www.cavalierdaily.com/page/faculty-salary-2020) . The dataset used in our evaluation was a smaller, more specific dataset on UVA's School of Engineering & Applied Science Faculty. However, the information provided within this dataset only included the faculty's name, ID, department, and tenure (Tenured vs. On Track vs. AGF). 

The Cavalier Daily's dataset was used to merge the salary into the dataset based on name. However, some names were not matched up because some faculty members use nicknames or had a change in last name likely due to getting married recently. Faculty members who were not found in the Cavalier Daily were manually double checked and their salaries were added if the department, role, and first name or last name matched up.

Multiple online name and gender dictionaries were used in conjunction with a Python program to determine the gender of the faculty member based on their first name since this information was not readily available. If any of the dictionaries labeled the name as indeterminately female/male (aka a gender-neutral name like Alex), the faculty member was given a ? for gender even if another gender dictionary gave it a determinate female or male labeling. If the gender was not found in any of the datasets, it was given a NA for gender.

In our evaluation, we will only be looking at faculty members who have a determine female or male name.




### Reading in the Data

The data has previously been merged and cleaned using Python, but we are going to make sure all of the rows with incomplete data are filtered out. Specifically, we will be filtering out employees with a gender the Python program was unable to find in any existing gender-name dictionaries or was deemed to be a gender-neutral name like Alex.

```{r, echo=FALSE, include=FALSE, warning=FALSE}

data <- read.csv("SEAS_cleandata.csv")
data <- subset(data, Gender %in% c("F", "M"))

#levels(data$Gender)<- c(NA, 0,1)
#data$Gender <- as.numeric(as.character(data$Gender))
vars <- c('Department', 'Position', 'Tenure','Gender', 'Salary')
data_clean <- data[,(names(data) %in% vars)]
data_clean$Salary <- as.numeric(gsub("[\\$,]", "", data_clean$Salary))

sapply(data_clean, class) 
```

```{r}
kable(head(data))
```

### Female vs. Male Split
To determine the split, we had to determine the ratio of female to total, and based on that, we can deduce the percent of male entries, and the percentage of both. To do this we created the function ad_split, which prints the ratio of female to total entries and ran it on the clean salary dataset. The split between female to male is 22 percent to 78 percent within the School of Engineering, respectively.

```{r echo=FALSE, include = FALSE, message = FALSE}
# Determine the split between female and male, then calculate the base rate
ad_split <- function(dat){
  female <- filter(dat, Gender == 'F')
  n_comm <- nrow(female)
  n_noncomm <- nrow(dat) - nrow(female)
  n_total <- nrow(dat)
  n_comm/n_total
}
ad_split(data)
#There is a 22/78 split
```

### Cleaning the Data

To filter out the vars, we create a list of the vars names and saved the dataset over itself without the columns in the vars variable. 
```{r echo=FALSE, message = FALSE}
# We drop all columns that do not contain relevant data
vars <- c('Department', 'Position', 'Tenure', 'Gender', 'Salary')
data_clean <- data[,(names(data) %in% vars)]
data_clean$Department <- as.numeric(as.factor(data_clean$Department))
data_clean$Position <- as.numeric(as.factor(data_clean$Position))
data_clean$Tenure <- as.numeric(as.factor(data_clean$Tenure))
data_clean$Gender <- as.numeric(as.factor(data_clean$Gender))
data_clean$Salary <- as.numeric(gsub("[\\$,]", "", data_clean$Salary))

sapply(data_clean, class) 
```

## Approach #1: KNN

James if you can explain how this would have worked here even though its not going to work-- worthwhile to give more info since we already did the work for it


Afterwards, we just simply had to run the function cor(), to discover how each column was related to each other to. We want to check if there are any highly correlated variables and remove them so as to not skew our KNN. We used a threshold of 0.7 to determine if something was too highly correlated.

### Correlation Between Variables
```{r}
# Before we run kNN, sometimes it's good to check to make sure that our variables are not highly correlated. Use the cor() function on data_clean, label it 'correlations', and view the data, because kNN doesn't work well in high dimensions. 
correlations <- cor(data_clean)
```

None of the variable are too highly correlated so we will leave these all in. We will note that Tenure and Salary has a correlation of 0.6 so we will keep an eye out for it. 

The good thing we can see here though is that Gender & Salary only have a correlation of 0.15 which is low and indicated that there isn't a glass ceiling we should be worried about if we are looking only at gender. However, there are more things that determine if there is indeed a glass ceiling-- specifically if these employees we are comparing are indeed doing "the same job".

### Generate Training and Testing Sets

```{r echo=FALSE, include = FALSE, message = FALSE}
# Use the index to generate a train and test sets, then check the row counts to be safe. 
# Check the composition of labels in the data set. 

# Let's split the data into a training and a test set.
# Sample 80% of our know data as training and 20% as test.
set.seed(1982)
train_rows <- sample(1:nrow(data_clean),
                              round(0.8 * nrow(data_clean), 0),
                              replace = FALSE)#<- don't replace the numbers


train_knn <- data_clean[train_rows, ] #<- select the rows identified
test_knn <- data_clean[-train_rows, ]  #<- select the rows that weren't identified 
```

### Data Sets {.tabset}

#### Training Data
```{r }
kable(head(train_knn))
```

#### Testing Data
```{r }
kable(head(test_knn))
```

### Choosing the Best K
```{r echo=FALSE, include = FALSE, message = FALSE}
# Run the "chooseK" function to find the perfect K, while using sapply() function on chooseK() to test k from 1 to 21 (only selecting the odd numbers)
labels<- colnames(data_clean)

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments#   If true, all distances equal to the kth largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy#could change this to Sensitivity 
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

knn_diff_k_com <- sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                                             train_set = train_knn[labels],
                                             val_set = test_knn[labels],
                                             train_class = train_knn[, "Salary"],
                                             val_class = test_knn[, "Salary"]))
# Create a dataframe so we can visualize the difference in accuracy based on K, convert the matrix to a dataframe
trans_knn_diff_k_com<-as.data.frame(t(as.matrix(knn_diff_k_com)))

colnames(trans_knn_diff_k_com)<-c("K", "Accuracy")
trans_knn_diff_k_com <- trans_knn_diff_k_com[order(trans_knn_diff_k_com$Accuracy),] 
trans_knn_diff_k_com
```

Unfortunately, we see that there are no good number of splits if we use KNN so instead we can look at which variable is most important using a decision tree!

## Approach #2: Decision Trees

We're going to switch over to using a decision tree to see if we can accurately predict an employee's salary based on a combination of 1-3 factors from their gender, department, and tenure.

### Cleaning the Data

To filter out the vars, we create a list of the vars names and saved the dataset over itself without the columns in the vars variable. 

Gender: Male is 0, Female is 1
```{r echo=FALSE, message = FALSE}
data <- read.csv("SEAS_cleandata.csv")[-63]
data <- subset(data, Gender %in% c("F", "M"))

data$Gender <- as.factor(data$Gender)
levels(data$Gender)<- c( 0,1)
data$Gender <- as.numeric(as.character(data$Gender))
vars <- c('Department', 'Position', 'Tenure','Gender', 'Salary')
data_tree <- data[,(names(data) %in% vars)]
data_tree$Salary <- as.numeric(gsub("[\\$,]", "", data_tree $Salary))

sapply(data_clean, class) 
```

### Splitting

We will start off our decision tree by splitting the data into a training and a testing set
```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Split your data into test and train using the caret
x <- createDataPartition(data_tree$Gender,times=1,p = 0.8,list=FALSE)
training_tree <- data_tree[x, -3]
testing_tree <- data_tree[-x, -3]
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
data_long = data_tree %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -Gender) 
data_long_form = ddply(data_long, 
                            .(Var, Value),#<- group by Var and Value, "." 
                            #allows us to call the variables without quoting
                            summarize,  
                            prob_Gender = mean(Gender), #<- probability of being Parent
                            prob_not_Gender = 1 - mean(Gender)) #<- probability of not being Parent
```

### Building the Model

We build our model using the default setting and by using the rpart function. PR.Status is used as the "formula" aka our response variable. We utilize our previously split training dataset and set a cp of 0.01 as it is our default.

```{r, echo=FALSE, include=FALSE, warning=FALSE}
set.seed(2702)
data_gini = rpart(Gender~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training_tree,#<- data used
                            control = rpart.control(cp=.01))
```


### Decision Tree{.tabset}
#### Visually 

First, we can take a look to see the order of importance of the employee's other factors that we can use to guess gender with.
```{r, echo=FALSE}
rpart.plot(data_gini, type =5, extra = 101)
```

Most important is the department that the employee is. If the employee is a member of the Computer Science or Biomedial, Chemical, Computer, Systems, or Mechanical Engineering department, the decision tree will guess that they are a female. From the testing data, we see that 78% of the employees in these departments are female.

If the employee is not in these departments, then if their salary is less than 77,000 they are labeled as a male by the decision tree. Otherwise if their salary is above 77,000 then they are labeled as a female by the decision tree.

#### Variable Importance

```{r, echo=FALSE}
kable(data_gini$variable.importance) 
```
In order of decreasing importance, department, salary, then position are the most important factors within the School of Engineering that can be used to accurately determine the faculty member's gender.

Ideally, the order would have been department, position, tenure, then salary. Department would be at the top of the list because this is for the most part, employee-chosen. Position would be next because this would also be driven by drive by the employee member and their background coming into UVA. Third is tenure which is similar to position, but has less to do with the employee's resume coming into UVA and moreso how UVA treats the employee throughout their time at UVA. If this factor was ranked higher, it would indicate that there is likely bias in UVA's own process at choosing professors for tenure. However, there is still a level of employee drive and initiative to even apply for being on track for tenure vs. staying as an AGF. Last is salary because salary is more reactive to the employee's current responsibilities and role at UVA. Tenure is similarly important, but can take 5-20 years for the university to make a binary choice versus the university reevaluating the professor's salary each year.

### Prediction

Next, we use the predict function to predict the target variable.
```{r, echo=FALSE}
tree_predict = predict(data_gini,testing_tree, type= "class")
```

### Confusion Matrix {.tabset}
#### Hit and Detection Rate
```{r, echo=FALSE, include=FALSE, warning=FALSE}
confusionMatrix(as.factor(tree_predict), as.factor(testing_tree$Gender), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
par_conf_matrix<-confusionMatrix(as.factor(tree_predict), as.factor(testing_tree$Gender), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```
The detection rate is found to be 66.7% with a prevalence of 73.8% Our model currently has an accuracy of 69.1%.

The hit rate can be derived from the confusion matrix by taking the number of true positives (28) over total actual positive cases (28+ a missing 3 = 31) for a hit rate of 90.3%.

```{r, echo=FALSE, include=FALSE, warning=FALSE}
par_error_rate = (par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]) / (par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]+par_conf_matrix$table[1,1]+par_conf_matrix$table[2,2])

par_error_rate
```
The Hit Rate/ True Error Rate is 30.1%

```{r, echo=FALSE, include=FALSE, warning=FALSE}
true_pos_rate = par_conf_matrix$table[2,2]/(par_conf_matrix$table[1,2]+par_conf_matrix$table[2,1]+par_conf_matrix$table[1,1]+par_conf_matrix$table[2,2])
true_pos_rate 
```
The true positive rate, also shown as our detection rate is 66.7%

The true positive rate could be improved upon as the ideal would create a confusion matrix with a value of 0 in the spot (1,2) also meaning a hit rate of 100%. Overall, this is a pretty good rate for our decision tree. The only concern would be a medium level of false positives (10 cases out of 42)

## Conclusion
We can make a pretty good decision tree that can determine gender based on department, tenure, salary, and position. This model has a hit rate of 90% which is fairly high.

As previously mentioned, it is good that salary and tenure were not highly ranked in variable importance, but the ideal order would have been department, position, tenure, then salary.

From the first approach, we can see that there are certain departments with high gender disparities which has more to do with employee choice of department rather than UVA being unfair and having a glass ceiling.
